\begin{abstract}

Training a speech recognition system needs audio data and their corresponding exact transcriptions. However, manual captioning is expensive, labor intensive, and error-prone. Some sources, such as: TV broadcast, have closed captions by default. Closed captions are closed to the exact transcription, but there are some paraphrasing, deletion, insertion, and substitution. Building automatic speech recognition from this data may result in a bad model. Therefore, selecting data is important to train a highly performance model. In this work, we explore the lightly supervised approach which is an iterative process to select good data. We built a baseline model and proposed new approaches.

For the baseline model, we select the data based on phone matched error rate and average word duration. The process to build the model runs iteratively by training different acoustic models and selecting data generated by the acoustic models. The model is evaluated by recognizing a development set. The development set comprises of audio data and their corresponding manually annotated transcriptions. The comparison between the decoding results on the development set and the manually annotated transcriptions produces word error rate which is the metric to measure how good the model is. We stop the data selection iterative process when the word error rate stops decreasing.

We proposed three new models: averaging word matched error rate and average word duration of 3 speech recognizers, combining 3 recognizers by our proposed algorithm, and utilizing random training set in each iteration. The error rate decreases in the combination proposed model as well as utilizing random training set. However, the proposed models perform slightly poorer in the next iteration.

 \textbf{Keyword: } automatic speech recognition, light supervised approach, error rate. 

%This internship used 200 hours of speech data and their corresponding closed captions obtained from BBC. However, only closed captions are provided by the challenge. In  other word, the closed captions are not exactly the same as what audio says. Utilizing the whole dataset generates a bad acoustic model owing to imperfect closed captions. Thus, the objective of the internship is to develop a data selection method to obtain a high performance automatic speech recognition(ASR). Simply put, the ASR must reduce word error rate as small as possible.

%A baseline system was developed from a 200 hours random subset of whole training set(train.200). Firstly, a deep neural network was trained on the 100 hours subset of data from train.200 and recognized train.200 to calculate phone matched error rate(PMER), word matched error rate(PMER), and average word duration(AWD). The audio segments were sorted and selected based on PMER and AWD for the next iteration. According to our experiments, baseline system shows decreasing trend of word error rate as well as phone matched error rate threshold. Moreover, I proposed a new algorithm which combines three ASR systems by varying language models. These novel approaches are compared with the baseline system to assess whether it achieves better performance.
\end{abstract}